{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNVkEfPYS6BqZqtn6bXA7fs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SadiaSharmin/RandomMethods/blob/main/Autoregression_AutoCorrelation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLRKraKIZ33k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "from matplotlib import pyplot \n",
        "from pandas.plotting import lag_plot, autocorrelation_plot\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "series = pd.read_csv('daily-min-temperatures.csv', header=0, index_col=0)\n",
        "print(series.head())\n",
        "series.plot()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "4xUdh3pHnD2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Below is an example of creating a lag plot of the Minimum Daily Temperatures dataset.\n",
        "#Lag plots are used to check if a data set or time series is random. Random data should not exhibit any structure in the lag plot. Non-random structure implies that the underlying data are not random.\n",
        "lag_plot(series)\n",
        "pyplot.show()\n",
        "#We can see a large ball of observations along a diagonal line of the plot. It clearly shows a relationship or some correlation."
      ],
      "metadata": {
        "id": "NM7YBEONqDTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This process could be repeated for any other lagged observation, such as if we wanted to review the relationship with the last 7 days or with the same day last month or last year.\n",
        "\n",
        "Another quick check that we can do is to directly calculate the correlation between the observation and the lag variable.\n",
        "\n",
        "We can use a statistical test like the Pearson correlation coefficient. This produces a number to summarize how correlated two variables are between -1 (negatively correlated) and +1 (positively correlated) with small values close to zero indicating low correlation and high values above 0.5 or below -0.5 showing high correlation.\n",
        "\n",
        "Correlation can be calculated easily using the corr() function on the DataFrame of the lagged dataset.\n",
        "\n",
        "The example below creates a lagged version of the Minimum Daily Temperatures dataset and calculates a correlation matrix of each column with other columns, including itself.\n",
        "\n"
      ],
      "metadata": {
        "id": "KGMpQw8FvUH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "values = pd.DataFrame(series.values)\n",
        "print(values)\n",
        "dataframe = pd.concat([values.shift(1), values], axis=1) #Combine DataFrame objects horizontally along the x axis by passing in axis=1.\n",
        "print(dataframe)\n",
        "dataframe.columns = ['t-1', 't+1']\n",
        "result = dataframe.corr() #Compute pairwise correlation of columns, excluding NA/null values.\n",
        "print(result)\n",
        "#It shows a strong positive correlation (0.77) between the observation and the lag=1 value."
      ],
      "metadata": {
        "id": "rYJwVLUHvVLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is good for one-off checks, but tedious if we want to check a large number of lag variables in our time series.\n",
        "\n",
        "Next, we will look at a scaled-up version of this approach.\n",
        "\n",
        "**Autocorrelation Plots** \\\\\n",
        "We can plot the correlation coefficient for each lag variable.\n",
        "\n",
        "This can very quickly give an idea of which lag variables may be good candidates for use in a predictive model and how the relationship between the observation and its historic values changes over time.\n",
        "\n",
        "We could manually calculate the correlation values for each lag variable and plot the result. Thankfully, Pandas provides a built-in plot called the autocorrelation_plot() function.\n",
        "\n",
        "The plot provides the lag number along the x-axis and the correlation coefficient value between -1 and 1 on the y-axis. The plot also includes solid and dashed lines that indicate the 95% and 99% confidence interval for the correlation values. Correlation values above these lines are more significant than those below the line, providing a threshold or cutoff for selecting more relevant lag values."
      ],
      "metadata": {
        "id": "WT0eDNOCy53e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autocorrelation_plot(series)\n",
        "pyplot.show()\n",
        "#Running the example shows the swing in positive and negative correlation as the temperature values change across summer and winter seasons each previous year."
      ],
      "metadata": {
        "id": "pVECJ9Zqy6s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The statsmodels library also provides a version of the plot in the plot_acf() function as a line plot.\n",
        "plot_acf(series, lags=31) #In this example, we limit the lag variables evaluated to 31 for readability. that's why all postive correlation\n",
        "#plot_acf(series, lags=500)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "6asK8kxJ3RXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know how to review the autocorrelation in our time series, let’s look at modeling it with an autoregression.\n",
        "\n",
        "Before we do that, let’s establish a baseline performance.\n",
        "\n",
        "**Persistence Model**  \\\\\n",
        "Let’s say that we want to develop a model to predict the last 7 days of minimum temperatures in the dataset given all prior observations.\n",
        "\n",
        "The simplest model that we could use to make predictions would be to persist the last observation. We can call this a persistence model and it provides a baseline of performance for the problem that we can use for comparison with an autoregression model.\n",
        "\n",
        "We can develop a test harness for the problem by splitting the observations into training and test sets, with only the last 7 observations in the dataset assigned to the test set as “unseen” data that we wish to predict.\n",
        "\n",
        "The predictions are made using a walk-forward validation model so that we can persist the most recent observations for the next day. This means that we are not making a 7-day forecast, but 7 1-day forecasts.\n",
        "\n"
      ],
      "metadata": {
        "id": "MffKwFmZ4cSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create lagged dataset\n",
        "values = pd.DataFrame(series.values)\n",
        "dataframe = pd.concat([values.shift(1), values], axis=1)\n",
        "dataframe.columns = ['t-1', 't+1']\n",
        "# split into train and test sets\n",
        "X = dataframe.values\n",
        "train, test = X[1:len(X)-7], X[len(X)-7:]\n",
        "train_X, train_y = train[:,0], train[:,1]\n",
        "test_X, test_y = test[:,0], test[:,1]\n",
        "\n",
        "# persistence model\n",
        "def model_persistence(x):\n",
        "\treturn x\n",
        "\n",
        "# walk-forward validation\n",
        "predictions = list()\n",
        "for x in test_X:\n",
        "\tyhat = model_persistence(x)\n",
        "\tpredictions.append(yhat)\n",
        " \n",
        "test_score = mean_squared_error(test_y, predictions)\n",
        "print('Test MSE: %.3f' % test_score)\n",
        "# plot predictions vs expected\n",
        "pyplot.plot(test_y)\n",
        "pyplot.plot(predictions, color='red')\n",
        "pyplot.show()\n",
        "#Running the example prints the mean squared error (MSE). The value provides a baseline performance for the problem.\n",
        "#The expected values for the next 7 days are plotted (blue) compared to the predictions from the model (red)."
      ],
      "metadata": {
        "id": "F5GHfQdX5JI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Autoregression Model**\\\\\n",
        "An autoregression model is a linear regression model that uses lagged variables as input variables.\n",
        "\n",
        "We could calculate the linear regression model manually using the LinearRegession class in scikit-learn and manually specify the lag input variables to use.\n",
        "\n",
        "Alternately, the statsmodels library provides an autoregression model where you must specify an appropriate lag value and trains a linear regression model. It is provided in the AutoReg class.\n",
        "\n",
        "We can use this model by first creating the model AutoReg() and then calling fit() to train it on our dataset. This returns an AutoRegResults object.\n",
        "\n",
        "Once fit, we can use the model to make a prediction by calling the predict() function for a number of observations in the future. This creates 1 7-day forecast, which is different from the persistence example above.\n",
        "\n",
        "The complete example is listed below."
      ],
      "metadata": {
        "id": "l2_tJFXA7V2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create and evaluate a static autoregressive model\n",
        "# load dataset\n",
        "series = pd.read_csv('daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
        "# split dataset\n",
        "X = series.values\n",
        "train, test = X[1:len(X)-7], X[len(X)-7:]\n",
        "# train autoregression\n",
        "model = AutoReg(train, lags=1) # using last 29 values for prediction\n",
        "model_fit = model.fit()\n",
        "print(model_fit.summary())\n",
        "print('Coefficients: %s' % model_fit.params)\n",
        "# make predictions\n",
        "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
        "for i in range(len(predictions)):\n",
        "\tprint('predicted=%f, expected=%f' % (predictions[i], test[i]))\n",
        "rmse = sqrt(mean_squared_error(test, predictions))\n",
        "print('Test RMSE: %.3f' % rmse)\n",
        "# plot results\n",
        "pyplot.plot(test)\n",
        "pyplot.plot(predictions, color='red')\n",
        "pyplot.show()\n",
        "#Running the example the list of coefficients in the trained linear regression model.\n",
        "#The 7 day forecast is then printed and the mean squared error of the forecast is summarized.\n",
        "#A plot of the expected (blue) vs the predicted values (red) is made. The forecast does look pretty good (about 1 degree Celsius out each day), with big deviation on day 5."
      ],
      "metadata": {
        "id": "MG3J6kOX7cId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statsmodels API does not make it easy to update the model as new observations become available.\n",
        "\n",
        "One way would be to re-train the AutoReg model each day as new observations become available, and that may be a valid approach, if not computationally expensive.\n",
        "\n",
        "An alternative would be to use the learned coefficients and manually make predictions. This requires that the history of 29 prior observations be kept and that the coefficients be retrieved from the model and used in the regression equation to come up with new forecasts.\n",
        "\n",
        "The coefficients are provided in an array with the intercept term followed by the coefficients for each lag variable starting at t-1 to t-n. We simply need to use them in the right order on the history of observations"
      ],
      "metadata": {
        "id": "khkqor_aB7rF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bmyjTd2zB-qj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}